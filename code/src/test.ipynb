{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/dlf/prompt/code/model/bert_large_chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Found cached dataset imdb (/home/dlf/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 399.18it/s]\n",
      "Parameter 'function'=<function tokenize_function at 0x7f426aa421f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "thread '<unnamed>' panicked at 'no entry found for key', tokenizers-lib/src/models/mod.rs:36:66\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'no entry found for key', tokenizers-lib/src/models/mod.rs:36:66\n",
      "thread '<unnamed>' panicked at 'no entry found for key', tokenizers-lib/src/models/mod.rs:36:66\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 88544\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 86549\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 177861\n",
      "    })\n",
      "})\n",
      "{'input_ids': [580, 630, 9326, 8990, 8782, 630, 8892, 624, 9164, 12216, 597, 646, 13692, 8699, 9149, 8901, 10018, 9198, 9294, 9287, 10733, 8684, 8992, 8653, 11964, 11822, 9463, 10657, 9710, 12020, 13058, 11956, 8782, 8712, 12090, 8712, 10426, 10034, 13496, 8647, 8696, 9607, 598, 630, 12695, 9724, 9824, 9710, 8722, 10034, 8712, 10426, 9821, 12268, 8647, 8599, 642, 598, 640, 598, 624, 13319, 9289, 8597, 9377, 8712, 12790, 8675, 641, 11541, 8647, 8707, 11565, 9033, 13157, 596, 11615, 10740, 8837, 9294, 8700, 622, 12404, 8684, 11628, 8597, 11964, 12235, 10228, 586, 11964, 11822, 9463, 13036, 8657, 586, 630, 10632, 8915, 12122, 8647, 8707, 10714, 9033, 8809, 8901, 13540, 598, 612, 9054, 599, 614, 612, 9054, 599, 614, 8653, 637, 11059, 8789, 9948, 8782, 9152, 11956, 622, 10281, 640, 12201], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [580, 630, 9326, 8990, 8782, 630, 8892, 624, 9164, 12216, 597, 646, 13692, 8699, 9149, 8901, 10018, 9198, 9294, 9287, 10733, 8684, 8992, 8653, 11964, 11822, 9463, 10657, 9710, 12020, 13058, 11956, 8782, 8712, 12090, 8712, 10426, 10034, 13496, 8647, 8696, 9607, 598, 630, 12695, 9724, 9824, 9710, 8722, 10034, 8712, 10426, 9821, 12268, 8647, 8599, 642, 598, 640, 598, 624, 13319, 9289, 8597, 9377, 8712, 12790, 8675, 641, 11541, 8647, 8707, 11565, 9033, 13157, 596, 11615, 10740, 8837, 9294, 8700, 622, 12404, 8684, 11628, 8597, 11964, 12235, 10228, 586, 11964, 11822, 9463, 13036, 8657, 586, 630, 10632, 8915, 12122, 8647, 8707, 10714, 9033, 8809, 8901, 13540, 598, 612, 9054, 599, 614, 612, 9054, 599, 614, 8653, 637, 11059, 8789, 9948, 8782, 9152, 11956, 622, 10281, 640, 12201]}\n",
      "{'input_ids': [9651, 9092, 11143, 8618, 9290, 12476, 8990, 9096, 8647, 9462, 8853, 9851, 13212, 8597, 8707, 9462, 8937, 8650, 12790, 11520, 12491, 9853, 9588, 9532, 9041, 598, 8696, 9603, 8796, 11243, 9853, 13212, 8597, 8707, 11931, 12387, 8722, 12077, 10131, 8707, 10101, 10219, 13527, 9447, 9195, 8684, 10135, 12346, 9756, 8760, 9579, 8653, 8854, 11019, 8920, 640, 12201, 8989, 641, 10049, 13146, 8644, 9532, 10878, 9195, 9826, 9038, 9115, 12391, 8789, 12351, 8597, 12020, 8849, 9454, 8653, 10617, 8897, 8853, 8654, 11262, 8735, 13098, 8807, 8789, 12351, 8597, 8696, 8653, 10437, 12082, 598, 8696, 9294, 12757, 11408, 9454, 10219, 9038, 9115, 10238, 9229, 8597, 8735, 9028, 9651, 12948, 8658, 8842, 9312, 11028, 8597, 8684, 9290, 9548, 10049, 10011, 9532, 8653, 9456, 10687, 10968, 10440, 8760, 9038, 9115, 11609, 596], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [9651, 9092, 11143, 8618, 9290, 12476, 8990, 9096, 8647, 9462, 8853, 9851, 13212, 8597, 8707, 9462, 8937, 8650, 12790, 11520, 12491, 9853, 9588, 9532, 9041, 598, 8696, 9603, 8796, 11243, 9853, 13212, 8597, 8707, 11931, 12387, 8722, 12077, 10131, 8707, 10101, 10219, 13527, 9447, 9195, 8684, 10135, 12346, 9756, 8760, 9579, 8653, 8854, 11019, 8920, 640, 12201, 8989, 641, 10049, 13146, 8644, 9532, 10878, 9195, 9826, 9038, 9115, 12391, 8789, 12351, 8597, 12020, 8849, 9454, 8653, 10617, 8897, 8853, 8654, 11262, 8735, 13098, 8807, 8789, 12351, 8597, 8696, 8653, 10437, 12082, 598, 8696, 9294, 12757, 11408, 9454, 10219, 9038, 9115, 10238, 9229, 8597, 8735, 9028, 9651, 12948, 8658, 8842, 9312, 11028, 8597, 8684, 9290, 9548, 10049, 10011, 9532, 8653, 9456, 10687, 10968, 10440, 8760, 9038, 9115, 11609, 596]}\n",
      "chunk\n",
      "tensor([  580,   630,  9326,  8990,  8782,   630,  8892,   624,   582, 12216,\n",
      "          597,   646, 13692,  8699,  9149,  8901, 10018,   582,  9294,   582,\n",
      "        10733,   582,  8992,   582, 11964,   582,  9463, 10657,  9710, 12020,\n",
      "        13058, 11956,  8782,  8712, 12090,  8712, 10426, 10034, 13496,  8647,\n",
      "         8696,  9607,   598,   630, 12695,  9724,  9824,  9710,  8722, 10034,\n",
      "         8712, 10426,   582, 12268,  8647,  8599,   642,   598,   640,   598,\n",
      "          624, 13319,  9289,  8597,  9377,  8712, 12790,  8675,   641, 11541,\n",
      "         8647,  8707, 11565,  9033, 17102,   596, 11615,   582,  8837,  9294,\n",
      "         8700,   582,   582,  8684, 11628,  8597, 11964, 12235, 10228,   586,\n",
      "        11964, 11822,  9463, 13036,  8657,   586,   630, 10632,  8915,   582,\n",
      "         8647,  8707, 10714,  9033,  8809,  8901, 13540,   598,   612,  9054,\n",
      "          599,   614,   582,  9054,   582,   582,  8653,   637, 11059,  8789,\n",
      "         9948,  8782,  9152, 11956,   622, 10281,   640, 12201])\n",
      "\n",
      "'>>> [CLS] i rented i am c [MASK]ious - yellow from my video [MASK] be [MASK]use [MASK] all [MASK] con [MASK]versy that surrounded it when it was first released in 1967. i also heard that at first it was [MASK]ized by u. s. customs if it ever tried to enter this横, there [MASK]re being [MASK] [MASK] of films considered \" controversial \" i really [MASK]d to see this for myself. < br / > [MASK] br [MASK] [MASK] the plot is centered around a young swe'\n",
      "tensor([ 9651,  9092, 11143,  8618,  9290, 12476,  8990,  9096,  8647,  9462,\n",
      "         8853,  9851, 13212,  8597,  8707,  9462,  8937,   582, 12790, 11520,\n",
      "        12491,  9853,  9588,   582,  9041,   598,  8696,  9603,  8796, 11243,\n",
      "         9853, 13212,  8597,  8707, 11931, 12387,  8722, 12077, 10131,  8707,\n",
      "        10101, 10219,   582,  9447,  9195,   582, 10135, 12346,  9756,  8760,\n",
      "         9579,  8653,  8854, 11019,   582,   640, 12201,  8989,   641, 10049,\n",
      "        13146,  8644,  9532,   582,  9195,   582,  9038,  9115,   582,  8789,\n",
      "        12351,  8597, 12020,   582,   582,  8653, 10617,  8897,  8853,  8654,\n",
      "        11262,  5465, 13098,  8807,  8789, 12351,  8597,  8696,  8653,   582,\n",
      "        12082,   598,  8696,  9294, 12757, 11408,  9454, 10219,  9038,  9115,\n",
      "        10238,  9229,  8597,  8735,  9028,   582, 12948,   582,  8842,  9312,\n",
      "        11028,  8597,  8684,  9290,  9548,   582, 10011,  9532,  8653,  9456,\n",
      "        10687, 10968, 10440,  8760,  9038,   582, 11609,   596])\n",
      "\n",
      "'>>> ##dish drama student named lena who wants to lear [MASK] everything she can [MASK] life. in particular she wants to focus her attentions to making [MASK] sort [MASK] documentary on what the avera [MASK] swede thought about [MASK]rt [MASK] poli [MASK] issues su [MASK] [MASK] the vietnam war 竅 race issues in the [MASK] states. in between asking politicians and or [MASK]nar [MASK] denizens of stock [MASK]lm about their opinions on po [MASK]tics,'\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,BertForMaskedLM\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from data_processing import  get_all_data\n",
    "\n",
    "\n",
    "datas = ['在句子\"脉细弦\"中，词语\"脉\"的前文如果是由\"[CLS]\"词性的词语\"[CLS]\"来修饰，那么词语\"脉\"的词性是\"[MASK]\"→ NR', '在句子\"脉细弦\"中，词语\"细\"的前文如果是由\"NR\"词性的词语\"脉\"来修饰，那么词语\"细\"的词性是\"[MASK]\"→ VA', '在句子\"脉细弦\"中，词语\"弦\"的前文如果是由\"VA\"词性的词语\"细\"来修饰，那么词语\"弦\"的词性是\"[MASK]\"→ VA']\n",
    "\n",
    "model_checkpoint =  \"/home/dlf/prompt/code/model/bert_large_chinese\"\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# for item in datas:\n",
    "#     text = item.split(\"→\")[0]\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#     token_logits = model(**inputs).logits\n",
    "#     # Find the location of [MASK] and extract its logits\n",
    "#     mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "#     mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "#     # Pick the [MASK] candidates with the highest logits\n",
    "#     top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "#     for token in top_5_tokens:\n",
    "#         print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
    "\n",
    "import logddd\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(imdb_dataset)\n",
    "import numpy as np\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "chunk_size = 128\n",
    "# 将块的大小调整为统一的大小\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "print(lm_datasets)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.1)\n",
    "\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "    print(sample)\n",
    "\n",
    "print(\"chunk\")\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    # [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as \" teachers \". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\'[MASK] satire is much closer to reality than is \" teachers \". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'\n",
    "    # 获取到的是加了[]\n",
    "    print(chunk)\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
    "import collections\n",
    "import numpy as np\n",
    " \n",
    "from transformers import default_data_collator\n",
    " \n",
    "wwm_probability = 0.2\n",
    " \n",
    " \n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    " \n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    " \n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    " \n",
    "    return default_data_collator(features)\n",
    "\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "128\n",
      "128\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"batch\")\n",
    "# dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "print(batch.keys())\n",
    "# print(batch['labels'])\n",
    "print(len(batch[\"labels\"][0]))\n",
    "print(len(batch[\"input_ids\"][0]))\n",
    "print(type(batch))\n",
    "import logddd\n",
    "# logddd.log(batch[\"input_ids\"] == batch[\"labels\"])\n",
    "# for key in batch.keys():\n",
    "#     logddd.log(batch[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
